schema: '2.0'
stages:
  train_enzymes@gcn_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_sum
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_sum:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_sum
      md5: 5c89a496652a6c5dbdf7a0cc5c41bc42.dir
      size: 11391910
      nfiles: 30
  train_enzymes@gcn_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_mean
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_mean:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_mean
      md5: 1218bd051256358eb448bd348c37b28a.dir
      size: 10946958
      nfiles: 30
  train_enzymes@gcn_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_max
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_max:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_max
      md5: 3da88db0ea62996c2f00509f14d03032.dir
      size: 10701863
      nfiles: 30
  train_enzymes@gcn_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_virtual_node
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_virtual_node:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_virtual_node
      md5: 088f3f337025d97b23ba965dce6c0d45.dir
      size: 11006126
      nfiles: 30
  train_enzymes@gcn_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_deepsets_base
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_deepsets_base:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_deepsets_base
      md5: ba117fe2fe24d9a38839643529f4fc9c.dir
      size: 14306520
      nfiles: 30
  train_enzymes@gcn_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_deepsets_large
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_deepsets_large:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_deepsets_large
      md5: d9deea7ff23096051d91e1bf71557f85.dir
      size: 22848918
      nfiles: 30
  train_enzymes@gin_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_sum
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_sum:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_sum
      md5: d2471f63c58be74e3f36c3fcf1c997b8.dir
      size: 17478487
      nfiles: 30
  train_enzymes@gin_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_mean
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_mean:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_mean
      md5: 4c36dc347b1c1affdf6dccbd4bdadcd3.dir
      size: 17794509
      nfiles: 30
  train_enzymes@gin_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_max
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_max:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_max
      md5: 18722fe79cb2c9b5c97393b3c3025c57.dir
      size: 16536927
      nfiles: 30
  train_enzymes@gin_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_virtual_node
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_virtual_node:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_virtual_node
      md5: 21ae57d9646925d15041794b685bce5b.dir
      size: 17796271
      nfiles: 30
  train_enzymes@gin_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_deepsets_base
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_deepsets_base:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_deepsets_base
      md5: a2f2c278cba18888a94de655eae7ef73.dir
      size: 21895223
      nfiles: 30
  train_enzymes@gin_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_deepsets_large
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_deepsets_large:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_deepsets_large
      md5: 97079fdec56b741eee1d405b333653e7.dir
      size: 29157447
      nfiles: 30
  train_enzymes@gat_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_sum
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_sum:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_sum
      md5: ea5b37dd8c24d9229ea6b177589d8fb9.dir
      size: 10353746
      nfiles: 30
  train_enzymes@gat_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_mean
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_mean:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_mean
      md5: 3fb7e0db07157ffd4ad623574ed714b1.dir
      size: 10134501
      nfiles: 30
  train_enzymes@gat_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_max
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_max:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_max
      md5: 0ab4755d3f6fc67e9172043fd19c9446.dir
      size: 9966806
      nfiles: 30
  train_enzymes@gat_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_virtual_node
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_virtual_node:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_virtual_node
      md5: d54d32b12fc6bb3bf725de1f88f4cb08.dir
      size: 10078042
      nfiles: 30
  train_enzymes@gat_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_deepsets_base
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_deepsets_base:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_deepsets_base
      md5: 8993f243a6f0c571373745635bef7c97.dir
      size: 14208111
      nfiles: 30
  train_enzymes@gat_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_deepsets_large
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_deepsets_large:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_deepsets_large
      md5: 5010eaa304f790b756239d7e3279a1a0.dir
      size: 22599544
      nfiles: 30
  train_enzymes@gcn_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_dense
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_dense:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_dense
      md5: 5c0acf9717d6db297861131a34dcc661.dir
      size: 509369507
      nfiles: 30
  train_enzymes@gcn_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_gru
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_gru:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_gru
      md5: d7a873937c1ef06335a35f8facb20143.dir
      size: 21497779
      nfiles: 30
  train_enzymes@gin_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_dense
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_dense:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_dense
      md5: a7f41a0533fbfca890e3ba8f1616e55f.dir
      size: 515236207
      nfiles: 30
  train_enzymes@gin_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_gru
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_gru:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_gru
      md5: 5b53c4a510e257fe955310e71855a8d3.dir
      size: 27638563
      nfiles: 30
  train_enzymes@gat_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_dense
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_dense:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_dense
      md5: 7b4dee81f026b723807ba26925383b19.dir
      size: 509432190
      nfiles: 30
  train_enzymes@gat_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_gru
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_gru:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_gru
      md5: 8fc4a012bd547213bcdc8b551004defb.dir
      size: 21578034
      nfiles: 30
  train_enzymes@gcn_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_concat_r
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_concat_r:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_concat_r
      md5: c901b7c965fcc210d3bf201cc7c150a7.dir
      size: 15071363
      nfiles: 30
  train_enzymes@gcn_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_w_mean_r
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_w_mean_r:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_w_mean_r
      md5: 0862e89af0bcf3a6b65bb4f62923ac78.dir
      size: 10618347
      nfiles: 30
  train_enzymes@gcn_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_w_mean_r_proj
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_w_mean_r_proj:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_w_mean_r_proj
      md5: c562223c084ff4811abf461decc5dcf9.dir
      size: 13177074
      nfiles: 30
  train_enzymes@gcn_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_mean_pred
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_mean_pred:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_mean_pred
      md5: dfd8f4a7538f92605428ebf53804bd2a.dir
      size: 11101303
      nfiles: 30
  train_enzymes@gcn_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_w_mean_pred
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_w_mean_pred:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_w_mean_pred
      md5: 72fa4fbc8eed45b790fba8f6caea40fa.dir
      size: 10936107
      nfiles: 30
  train_enzymes@gcn_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gcn_w_mean_pred_proj
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gcn_w_mean_pred_proj:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gcn_w_mean_pred_proj
      md5: bc5d82126e8761de4cb286e2b986fc0e.dir
      size: 16604491
      nfiles: 30
  train_enzymes@gin_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_concat_r
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_concat_r:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_concat_r
      md5: 49794aa485a9e01cd44711c022032aa4.dir
      size: 21048279
      nfiles: 30
  train_enzymes@gin_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_w_mean_r
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_w_mean_r:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_w_mean_r
      md5: a6f95b37592285c6cff4c81db21f3cba.dir
      size: 17294451
      nfiles: 30
  train_enzymes@gin_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_w_mean_r_proj
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_w_mean_r_proj:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_w_mean_r_proj
      md5: 93eefc2eba2c1501dc352a19eb2abf48.dir
      size: 19436494
      nfiles: 30
  train_enzymes@gin_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_mean_pred
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_mean_pred:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_mean_pred
      md5: 098d56fda74a35fa4c0b06c7afd7a44d.dir
      size: 17703796
      nfiles: 30
  train_enzymes@gin_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_w_mean_pred
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_w_mean_pred:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_w_mean_pred
      md5: ebec140f58fd89b430fa8105e76487f3.dir
      size: 17180069
      nfiles: 30
  train_enzymes@gin_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gin_w_mean_pred_proj
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gin_w_mean_pred_proj:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gin_w_mean_pred_proj
      md5: 18244210e8ccd1c38092a3c7d9501a37.dir
      size: 23127770
      nfiles: 30
  train_enzymes@gat_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_concat_r
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_concat_r:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_concat_r
      md5: 6843cfb8f883b5c6955370287a9abb47.dir
      size: 14433040
      nfiles: 30
  train_enzymes@gat_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_w_mean_r
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_w_mean_r:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_w_mean_r
      md5: 0ca0ca9f6cd6d8dd79af9108a379fb3f.dir
      size: 10362993
      nfiles: 30
  train_enzymes@gat_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_w_mean_r_proj
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_w_mean_r_proj:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_w_mean_r_proj
      md5: 3c595ed5cb021aaedb96651ec6c129b3.dir
      size: 12407207
      nfiles: 30
  train_enzymes@gat_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_mean_pred
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_mean_pred:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_mean_pred
      md5: 620be8c8cb2fbcefca0f04bd6327d7fd.dir
      size: 10275960
      nfiles: 30
  train_enzymes@gat_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_w_mean_pred
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_w_mean_pred:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_w_mean_pred
      md5: cc7f3a7c7e5c7face2898cc30f900ea0.dir
      size: 10249245
      nfiles: 30
  train_enzymes@gat_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_enzymes.yaml --config-key gat_w_mean_pred_proj
    deps:
    - path: data/datasets/ENZYMES
      md5: e433f2c3d7dae3f4ad66213065b8f3d8.dir
      size: 7739831
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_enzymes.yaml:
        gat_w_mean_pred_proj:
          task_level: graph
          dataset_type: tud
          dataset_name: ENZYMES
          input_dim: 21
          output_dim: 6
          dataset_kwargs:
            use_node_attr: true
          max_nodes: 126
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          dataset_dir: data/datasets
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/ENZYMES/gat_w_mean_pred_proj
      md5: 5eef4ed7355f6da1d32d45e852d05eca.dir
      size: 16420264
      nfiles: 30
  train_reddit_multi@gcn_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_sum
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_sum:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_sum
      md5: 1206179f5808dfd86e571463a4caf02c.dir
      size: 60783447
      nfiles: 30
  train_reddit_multi@gcn_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_mean
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_mean:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_mean
      md5: 6ba8bbcd8b3e911cc8ad628aac80e293.dir
      size: 65329312
      nfiles: 30
  train_reddit_multi@gcn_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_max
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_max:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_max
      md5: 2b53fbcb5c4bc568716773dbaa8a5b6a.dir
      size: 63071627
      nfiles: 30
  train_reddit_multi@gcn_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_virtual_node
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_virtual_node:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
            CustomVirtualNode:
              connectivity: in
          model_name: GraphModel
          readout_name: VirtualNodeReadout
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_virtual_node
      md5: 7b86e9dd0d96601065a7f7e3cfff7bdf.dir
      size: 63226013
      nfiles: 30
  train_reddit_multi@gcn_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_deepsets_base
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_deepsets_base:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_deepsets_base
      md5: b4556a25eae7c8d31c2601bdf6e7995d.dir
      size: 65651523
      nfiles: 30
  train_reddit_multi@gcn_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_deepsets_large
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_deepsets_large:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_deepsets_large
      md5: c527c30eedaeec2f8c7cfb8a731b7d39.dir
      size: 74035663
      nfiles: 30
  train_reddit_multi@gin_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_sum
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_sum:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_sum
      md5: dc81764f130309c9fbc2aa4fcb5f08e5.dir
      size: 67465568
      nfiles: 30
  train_reddit_multi@gin_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_mean
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_mean:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_mean
      md5: aaf3051dcade806d362f0c516019de84.dir
      size: 66062177
      nfiles: 30
  train_reddit_multi@gin_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_max
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_max:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_max
      md5: a28eeace100d0c0301f86ea9aac9988f.dir
      size: 66641820
      nfiles: 30
  train_reddit_multi@gin_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_virtual_node
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_virtual_node:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
            CustomVirtualNode:
              connectivity: in
          model_name: GraphModel
          readout_name: VirtualNodeReadout
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_virtual_node
      md5: 39488bf4bcaea2b3a0b6649aa8c1f146.dir
      size: 66735193
      nfiles: 30
  train_reddit_multi@gin_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_deepsets_base
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_deepsets_base:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_deepsets_base
      md5: 4b998efef739b7d02d6dbfb8962c4eca.dir
      size: 71814491
      nfiles: 30
  train_reddit_multi@gin_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_deepsets_large
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_deepsets_large:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_deepsets_large
      md5: c1f17ba9b72f875ee991cd7b262db770.dir
      size: 79279082
      nfiles: 30
  train_reddit_multi@gat_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_sum
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_sum:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutSum
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_sum
      md5: 6ddf0038c3b077026fc0d526211a1748.dir
      size: 60378027
      nfiles: 30
  train_reddit_multi@gat_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_mean
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_mean:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutMean
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_mean
      md5: bcd9d2fd55f3e4bedbed911fc543e9c3.dir
      size: 62392291
      nfiles: 30
  train_reddit_multi@gat_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_max
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_max:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: AggregateReadoutMax
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_max
      md5: 75c5f53452029f7c16f679c65983d297.dir
      size: 61995003
      nfiles: 30
  train_reddit_multi@gat_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_virtual_node
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_virtual_node:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
            CustomVirtualNode:
              connectivity: in
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_virtual_node
      md5: 34261d55dff06f19b701508f51a8b69d.dir
      size: 62607022
      nfiles: 30
  train_reddit_multi@gat_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_deepsets_base
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_deepsets_base:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_deepsets_base
      md5: 82d120087fd7b0d39c9426ffbc0682aa.dir
      size: 64847940
      nfiles: 30
  train_reddit_multi@gat_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_deepsets_large
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_deepsets_large:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_deepsets_large
      md5: cb0c998c3f6e23a88d27c42f950f3615.dir
      size: 73503964
      nfiles: 30
  train_reddit_multi@gcn_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_dense
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_dense:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_dense
      md5: 13458f192c768168889161625f910eb5.dir
      size: 14935244785
      nfiles: 30
  train_reddit_multi@gcn_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_gru
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_gru:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_gru
      md5: d52bb136b66f566a04a60c3a33fffbca.dir
      size: 240361026
      nfiles: 30
  train_reddit_multi@gin_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_dense
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_dense:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_dense
      md5: bfe50c53da05357b1d0d9b0da3e7c60f.dir
      size: 14941407756
      nfiles: 30
  train_reddit_multi@gin_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_gru
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_gru:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_gru
      md5: 8686ca38e48063ace76889d80eae086f.dir
      size: 246063012
      nfiles: 30
  train_reddit_multi@gat_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_dense
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_dense:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_dense
      md5: a14a371be6eefaa65975905d41e404f6.dir
      size: 14935355761
      nfiles: 30
  train_reddit_multi@gat_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_gru
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_gru:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_gru
      md5: b6c2195f9596a2c769176604eec6ebc0.dir
      size: 240410297
      nfiles: 30
  train_reddit_multi@gcn_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_concat_r
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_concat_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_concat_r
      md5: adaf4298a07e3f2d5b6e8376332804d8.dir
      size: 65117733
      nfiles: 30
  train_reddit_multi@gcn_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_w_mean_r
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_w_mean_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_w_mean_r
      md5: d8aecf2a0cd71a42121010315e2c3059.dir
      size: 61121168
      nfiles: 30
  train_reddit_multi@gcn_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_w_mean_r_proj
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_w_mean_r_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_w_mean_r_proj
      md5: 73f910c8b2d5127045d12f350d0a353f.dir
      size: 63369931
      nfiles: 30
  train_reddit_multi@gcn_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_mean_pred
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_mean_pred
      md5: a4355020c8aea53a65c5d4864c0786da.dir
      size: 61338663
      nfiles: 30
  train_reddit_multi@gcn_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_w_mean_pred
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_w_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_w_mean_pred
      md5: 021267a47deee475f053ff84d787d78c.dir
      size: 62160803
      nfiles: 30
  train_reddit_multi@gcn_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gcn_w_mean_pred_proj
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gcn_w_mean_pred_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gcn_w_mean_pred_proj
      md5: 1af041d962fe3e57de27a2f2a7a7291b.dir
      size: 67505602
      nfiles: 30
  train_reddit_multi@gin_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_concat_r
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_concat_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_concat_r
      md5: fb1602c3370aab44fd6cea8e9667d1b0.dir
      size: 70427339
      nfiles: 30
  train_reddit_multi@gin_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_w_mean_r
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_w_mean_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_w_mean_r
      md5: 0b1ec7395bc77775419e95db2fde098b.dir
      size: 67097734
      nfiles: 30
  train_reddit_multi@gin_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_w_mean_r_proj
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_w_mean_r_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_w_mean_r_proj
      md5: 6e6db6a53029883efd398eb829a8aa15.dir
      size: 68524687
      nfiles: 30
  train_reddit_multi@gin_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_mean_pred
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_mean_pred
      md5: bd9cf54b781aa0166e41837970f258b5.dir
      size: 66830924
      nfiles: 30
  train_reddit_multi@gin_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_w_mean_pred
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_w_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_w_mean_pred
      md5: 2299f7194ac2605686a51d529434e40a.dir
      size: 66367854
      nfiles: 30
  train_reddit_multi@gin_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gin_w_mean_pred_proj
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gin_w_mean_pred_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gin_w_mean_pred_proj
      md5: bdfef701b8f7d5a8a9f89c4c1476cd63.dir
      size: 72568910
      nfiles: 30
  train_reddit_multi@gat_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_concat_r
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_concat_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_concat_r
      md5: 7ce5e1575b75c957ebb7354d11c9ac4b.dir
      size: 64468625
      nfiles: 30
  train_reddit_multi@gat_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_w_mean_r
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_w_mean_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_w_mean_r
      md5: ab40f02c6a3d641641d6d6632c822f6e.dir
      size: 60742668
      nfiles: 30
  train_reddit_multi@gat_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_w_mean_r_proj
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_w_mean_r_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_w_mean_r_proj
      md5: d497748d347b986c3f90d8a5050a2538.dir
      size: 62778258
      nfiles: 30
  train_reddit_multi@gat_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_mean_pred
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_mean_pred
      md5: 5bffc60b10299c729271946b81c9d381.dir
      size: 60750828
      nfiles: 30
  train_reddit_multi@gat_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_w_mean_pred
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_w_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_w_mean_pred
      md5: 2fad3fded02cb44a3831c112487bc831.dir
      size: 60592204
      nfiles: 30
  train_reddit_multi@gat_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml --config-key
      gat_w_mean_pred_proj
    deps:
    - path: data/datasets/REDDIT-MULTI-12K
      md5: 77ef2f9b5a2234362184cece1a30c3c8.dir
      size: 559182572
      nfiles: 6
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_reddit_multi.yaml:
        gat_w_mean_pred_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: REDDIT-MULTI-12K
          input_dim: 3063
          output_dim: 11
          max_nodes: 3782
          max_degree: 3062
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          transforms_config:
            OneHotDegree:
              max_degree: ${reddit_multi.max_degree}
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K/gat_w_mean_pred_proj
      md5: 7cef2fb45970d324697e0d64eee27565.dir
      size: 67127552
      nfiles: 30
  train_zinc@gcn_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_sum
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_sum:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_sum
      md5: bdfb77b79c604d10510f866d507b8a11.dir
      size: 16461340
      nfiles: 30
  train_zinc@gcn_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_mean
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_mean:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_mean
      md5: 5d873e7ba4fb28f641366892ce3d4ff6.dir
      size: 21998103
      nfiles: 30
  train_zinc@gcn_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_max
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_max:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_max
      md5: a40137e1e04e6cdb74ca4fa32fa582bf.dir
      size: 14802432
      nfiles: 30
  train_zinc@gcn_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_virtual_node
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_virtual_node:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_virtual_node
      md5: cc8e9ad7a09c64866f7fc884f4585ce8.dir
      size: 21345738
      nfiles: 30
  train_zinc@gcn_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_deepsets_base
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_deepsets_base:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_deepsets_base
      md5: a683cca68746471ac13c4e12091ce8cf.dir
      size: 23991607
      nfiles: 30
  train_zinc@gcn_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_deepsets_large
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_deepsets_large:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_deepsets_large
      md5: dbeb71c8157e1a8696d4d6ce3b877d8e.dir
      size: 29251814
      nfiles: 30
  train_zinc@gin_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_sum
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_sum:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_sum
      md5: aaa5998a07b2478826f5ca0a75f87c29.dir
      size: 22382765
      nfiles: 30
  train_zinc@gin_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_mean
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_mean:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_mean
      md5: 7e0bd5d6368c42bdeff4a4882e1d6402.dir
      size: 23423380
      nfiles: 30
  train_zinc@gin_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_max
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_max:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_max
      md5: 03f1339217c216a1eec2170e735a8cd1.dir
      size: 21372338
      nfiles: 30
  train_zinc@gin_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_virtual_node
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_virtual_node:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_virtual_node
      md5: 26687e6bb8fff77998c746964dd4c403.dir
      size: 21735438
      nfiles: 30
  train_zinc@gin_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_deepsets_base
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_deepsets_base:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_deepsets_base
      md5: c909e58200505aef3184907ec954433b.dir
      size: 25371036
      nfiles: 30
  train_zinc@gin_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_deepsets_large
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_deepsets_large:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_deepsets_large
      md5: ed79762af5e012835971db7bf706eaa8.dir
      size: 38547170
      nfiles: 30
  train_zinc@gat_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_sum
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_sum:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_sum
      md5: 56e739df088f85256ced1d69f3e693f2.dir
      size: 17993523
      nfiles: 30
  train_zinc@gat_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_mean
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_mean:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_mean
      md5: c68dd1415edba13837c6ebbc633cf56c.dir
      size: 21070068
      nfiles: 30
  train_zinc@gat_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_max
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_max:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_max
      md5: 4c2643314f63628b86165793952f77be.dir
      size: 14057382
      nfiles: 30
  train_zinc@gat_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_virtual_node
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_virtual_node:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_virtual_node
      md5: e446c6e919f1c32285c87c467a8b97b8.dir
      size: 18779527
      nfiles: 30
  train_zinc@gat_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_deepsets_base
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_deepsets_base:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_deepsets_base
      md5: f66df2362123cdbdf5b3fce195026f1e.dir
      size: 21146373
      nfiles: 30
  train_zinc@gat_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_deepsets_large
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_deepsets_large:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 128
            output_dim: 128
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_deepsets_large
      md5: 9005af4b1d41c82a573d3a796051ec14.dir
      size: 28215339
      nfiles: 30
  train_zinc@gcn_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_dense
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_dense:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_dense
      md5: 24f432b52d018059137b322fb594b74a.dir
      size: 165686719
      nfiles: 30
  train_zinc@gcn_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_gru
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_gru:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_gru
      md5: 305df0521ff7ebc6a25f1a6eac431011.dir
      size: 19674837
      nfiles: 30
  train_zinc@gin_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_dense
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_dense:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_dense
      md5: 8f37a64efdc724b13c84a3f00e773505.dir
      size: 172466626
      nfiles: 30
  train_zinc@gin_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_gru
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_gru:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_gru
      md5: 5e23d4fa40baa92b60b1b5f3307016c9.dir
      size: 25036984
      nfiles: 30
  train_zinc@gat_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_dense
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_dense:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_dense
      md5: 50328a630b9503c367e2acb06013dee6.dir
      size: 165602152
      nfiles: 30
  train_zinc@gat_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_gru
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_gru:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_gru
      md5: bff25e1c2f425ab23270d868ef3c7874.dir
      size: 20300880
      nfiles: 30
  train_zinc@gcn_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_concat_r
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_concat_r:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_concat_r
      md5: c4505786cd3c49c68b278b1d93dc3bc9.dir
      size: 18724764
      nfiles: 30
  train_zinc@gcn_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_w_mean_r
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_w_mean_r:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_w_mean_r
      md5: 23bc46662bb34232e4e9c2dc063db8b0.dir
      size: 16505848
      nfiles: 30
  train_zinc@gcn_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_w_mean_r_proj
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_w_mean_r_proj:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_w_mean_r_proj
      md5: 40eec4bbe952cabfd8039d86e00dc19b.dir
      size: 19750384
      nfiles: 30
  train_zinc@gcn_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_mean_pred
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_mean_pred:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_mean_pred
      md5: 821e33aae947e0c9c3827ce88c82a554.dir
      size: 17040471
      nfiles: 30
  train_zinc@gcn_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_w_mean_pred
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_w_mean_pred:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegressionEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_w_mean_pred
      md5: 2ea984a926520b1bde597f95a8c817d3.dir
      size: 17639258
      nfiles: 30
  train_zinc@gcn_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gcn_w_mean_pred_proj
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gcn_w_mean_pred_proj:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegressionEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gcn_w_mean_pred_proj
      md5: 1809fb3d8ee902321f689ad06fbe77ef.dir
      size: 21909696
      nfiles: 30
  train_zinc@gin_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_concat_r
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_concat_r:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_concat_r
      md5: 719d3c29f36ec91ddd83fb64d21f0b98.dir
      size: 28069575
      nfiles: 30
  train_zinc@gin_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_w_mean_r
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_w_mean_r:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_w_mean_r
      md5: f0309c10e23b6d8cb012b3ce0b8fe41a.dir
      size: 25036484
      nfiles: 30
  train_zinc@gin_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_w_mean_r_proj
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_w_mean_r_proj:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_w_mean_r_proj
      md5: 8985870a72a534b6fdb1185a0009d8f4.dir
      size: 25774816
      nfiles: 30
  train_zinc@gin_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_mean_pred
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_mean_pred:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_mean_pred
      md5: ceaacded18a9b0373186da11b58f57c5.dir
      size: 21755411
      nfiles: 30
  train_zinc@gin_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_w_mean_pred
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_w_mean_pred:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegressionEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_w_mean_pred
      md5: 71620fab28a175f317c11823df7a61d1.dir
      size: 22609295
      nfiles: 30
  train_zinc@gin_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gin_w_mean_pred_proj
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gin_w_mean_pred_proj:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegressionEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gin_w_mean_pred_proj
      md5: 5223b22c3f04ba9e232a0a87b14d8c0c.dir
      size: 31145831
      nfiles: 30
  train_zinc@gat_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_concat_r
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_concat_r:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 384
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_concat_r
      md5: cc7c4c11835867247466714911cabaac.dir
      size: 18024956
      nfiles: 30
  train_zinc@gat_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_w_mean_r
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_w_mean_r:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_w_mean_r
      md5: e9ac5539a3261455cbb7ce73da14cb1c.dir
      size: 17147610
      nfiles: 30
  train_zinc@gat_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_w_mean_r_proj
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_w_mean_r_proj:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegression
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_w_mean_r_proj
      md5: a5767e6dc54fb98fbdbadabc6ea21e88.dir
      size: 19830949
      nfiles: 30
  train_zinc@gat_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_mean_pred
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_mean_pred:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_mean_pred
      md5: e4fce0434ff40dc51828c4c2b9106ef8.dir
      size: 16672444
      nfiles: 30
  train_zinc@gat_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_w_mean_pred
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_w_mean_pred:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegressionEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_w_mean_pred
      md5: 2766a1fde229d20804a37179e8b66f2f.dir
      size: 17330917
      nfiles: 30
  train_zinc@gat_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_zinc.yaml --config-key gat_w_mean_pred_proj
    deps:
    - path: data/datasets/ZINC
      md5: e3260ea069979b0f56c12462066d9cb6.dir
      size: 754727543
      nfiles: 18
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_zinc.yaml:
        gat_w_mean_pred_proj:
          dataset_dir: data/datasets/ZINC
          task_level: graph
          dataset_type: zinc
          dataset_name: zinc
          input_dim: 28
          output_dim: 1
          dataset_kwargs:
            subset: true
          max_nodes: 38
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 128
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_R2
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 128
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPRegressionEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/zinc/gat_w_mean_pred_proj
      md5: 3baa76b8099a6c47927bdba97ef04d75.dir
      size: 20782559
      nfiles: 30
  train_mutag@gcn_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_sum
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_sum:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_sum
      md5: a9a6744eeb8b31fac4ab880f034bb9ba.dir
      size: 5068102
      nfiles: 30
  train_mutag@gcn_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_mean
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_mean:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_mean
      md5: 795b725cccf0777c1ddc77cebf764816.dir
      size: 4958359
      nfiles: 30
  train_mutag@gcn_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_max
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_max:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_max
      md5: b1a5e9947fcfc3d8d8c6e7049142aa35.dir
      size: 4899666
      nfiles: 30
  train_mutag@gcn_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_virtual_node
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_virtual_node:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_virtual_node
      md5: b9c57b3831b8700503460a36e5e63abb.dir
      size: 5181962
      nfiles: 30
  train_mutag@gcn_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_deepsets_base
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_deepsets_base:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 64
            output_dim: 64
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_deepsets_base
      md5: 3ac38af21665042fb1340fc89920464c.dir
      size: 6538761
      nfiles: 30
  train_mutag@gcn_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_deepsets_large
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_deepsets_large:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 64
            output_dim: 64
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_deepsets_large
      md5: 7d407d0f29e752a81606c051cf4b5dd7.dir
      size: 8447944
      nfiles: 30
  train_mutag@gin_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_sum
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_sum:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_sum
      md5: 896f7d46e065481eb9b7a21532847d02.dir
      size: 7121968
      nfiles: 30
  train_mutag@gin_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_mean
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_mean:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_mean
      md5: 79f8eb0b6bca89c2cc1d787263dc3c53.dir
      size: 7270695
      nfiles: 30
  train_mutag@gin_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_max
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_max:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_max
      md5: cfa9b96ac2188f07ed5d3ac7b878c417.dir
      size: 7082697
      nfiles: 30
  train_mutag@gin_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_virtual_node
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_virtual_node:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_virtual_node
      md5: f27abef8a91b7ce0ee00d8a0a9268630.dir
      size: 6818313
      nfiles: 30
  train_mutag@gin_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_deepsets_base
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_deepsets_base:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 64
            output_dim: 64
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_deepsets_base
      md5: 4cfaa9c62855be5476356f54eec1ef3f.dir
      size: 7767160
      nfiles: 30
  train_mutag@gin_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_deepsets_large
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_deepsets_large:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 64
            output_dim: 64
            activation: relu
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_deepsets_large
      md5: 3ae89b8a468d6a2295fa039923d9f4cb.dir
      size: 10016868
      nfiles: 30
  train_mutag@gat_sum:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_sum
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_sum:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutSum
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_sum
      md5: a645439158af7f4b936bebf4ceab6a9d.dir
      size: 5145692
      nfiles: 30
  train_mutag@gat_mean:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_mean
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_mean:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMean
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_mean
      md5: 172dda5d8d21b566a594bc114358fa17.dir
      size: 5460209
      nfiles: 30
  train_mutag@gat_max:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_max
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_max:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: AggregateReadoutMax
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_max
      md5: b268957839b4ce4d4ed0fbb7974d7653.dir
      size: 4930869
      nfiles: 30
  train_mutag@gat_virtual_node:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_virtual_node
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_virtual_node:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: VirtualNodeReadout
          transforms_config:
            CustomVirtualNode:
              connectivity: in
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_virtual_node
      md5: 2aa22b3f47046f89d871e7ab45be8651.dir
      size: 5461626
      nfiles: 30
  train_mutag@gat_deepsets_base:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_deepsets_base
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_deepsets_base:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsBase
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 64
            output_dim: 64
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_deepsets_base
      md5: 31cfc122fea9158863dd2bffa85c1d0c.dir
      size: 6362296
      nfiles: 30
  train_mutag@gat_deepsets_large:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_deepsets_large
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_deepsets_large:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DeepSetsLarge
          readout_init_args:
            input_dim: ${default.repr_dim}
            intermediate_dim: 64
            output_dim: 64
            activation: relu
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_deepsets_large
      md5: 655465cefc3c08dd01d8a268657ec8ef.dir
      size: 8596004
      nfiles: 30
  train_mutag@gcn_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_dense
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_dense:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_dense
      md5: 39e3154bb27123fd3e22c224c41e9e18.dir
      size: 64960005
      nfiles: 30
  train_mutag@gcn_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_gru
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_gru:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_gru
      md5: c3e4e612a336047df942d19c836fb3d1.dir
      size: 6916908
      nfiles: 30
  train_mutag@gin_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_dense
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_dense:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_dense
      md5: 0e4927d9699bb34ac9b64ea211628cc3.dir
      size: 66512664
      nfiles: 30
  train_mutag@gin_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_gru
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_gru:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_gru
      md5: e57bf2ace96a8d654d2e4205d2b7ad48.dir
      size: 8468793
      nfiles: 30
  train_mutag@gat_dense:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_dense
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_dense:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.output_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: DenseReadout
          readout_init_args:
            max_num_nodes_in_graph: ${..max_nodes}
            input_dim: ${..repr_dim}
            intermediate_dim: 256
            output_dim: 128
            dropout_rate: 0.4
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_dense
      md5: e509063c7a6e8a99dc4a638e2e7f029b.dir
      size: 65091268
      nfiles: 30
  train_mutag@gat_gru:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_gru
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_gru:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.readout_init_args.hidden_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: GRUReadout
          readout_init_args:
            hidden_dim: ${..repr_dim}
            max_num_nodes_in_graph: ${..max_nodes}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_gru
      md5: 90220411a83cceca6f1964faa7d8591c.dir
      size: 7117503
      nfiles: 30
  train_mutag@gcn_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_concat_r
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_concat_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 192
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_concat_r
      md5: bac8e568a2b82ea5c37e6f94b31a6cf9.dir
      size: 6875813
      nfiles: 30
  train_mutag@gcn_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_w_mean_r
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_w_mean_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_w_mean_r
      md5: d408281f19d9c9109e4885ca4f1544f5.dir
      size: 4731999
      nfiles: 30
  train_mutag@gcn_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_w_mean_r_proj
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_w_mean_r_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_w_mean_r_proj
      md5: c567ad5479286b2f11b9b4a4cb0a3799.dir
      size: 5422941
      nfiles: 30
  train_mutag@gcn_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_mean_pred
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_mean_pred
      md5: cb9327ec57fb5801c119c30851fc56c4.dir
      size: 5093228
      nfiles: 30
  train_mutag@gcn_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_w_mean_pred
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_w_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_w_mean_pred
      md5: 4c45e1a0ecb6d0c816d75098524de33a.dir
      size: 4902583
      nfiles: 30
  train_mutag@gcn_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gcn_w_mean_pred_proj
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gcn_w_mean_pred_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gcn
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gcn_w_mean_pred_proj
      md5: a13ed22e7a17199026f87631860150fa.dir
      size: 6501376
      nfiles: 30
  train_mutag@gin_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_concat_r
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_concat_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 192
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_concat_r
      md5: 4bd1bf013adaceae011f568b147f17a4.dir
      size: 8777842
      nfiles: 30
  train_mutag@gin_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_w_mean_r
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_w_mean_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_w_mean_r
      md5: baec32497624a9ce40c694ce9af03f88.dir
      size: 6893807
      nfiles: 30
  train_mutag@gin_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_w_mean_r_proj
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_w_mean_r_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_w_mean_r_proj
      md5: de8cd34b0e1e2e2628e525f33ff5aea6.dir
      size: 7310871
      nfiles: 30
  train_mutag@gin_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_mean_pred
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_mean_pred
      md5: b122803c41917b639a9913a29633a739.dir
      size: 7039347
      nfiles: 30
  train_mutag@gin_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_w_mean_pred
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_w_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_w_mean_pred
      md5: c035c5d9a685f580b5a0908d49432cf6.dir
      size: 6779069
      nfiles: 30
  train_mutag@gin_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gin_w_mean_pred_proj
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gin_w_mean_pred_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gin
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gin_w_mean_pred_proj
      md5: 34edb2b08c86a5b561fbc5de48d7553b.dir
      size: 8186924
      nfiles: 30
  train_mutag@gat_concat_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_concat_r
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_concat_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: 192
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsConcat
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_concat_r
      md5: 5c5afbe2c57db92710ca4448dc460a77.dir
      size: 6999743
      nfiles: 30
  train_mutag@gat_w_mean_r:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_w_mean_r
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_w_mean_r:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsWeightedCombine
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_w_mean_r
      md5: 3806041f6660471ceb0c1e9d7869823e.dir
      size: 5037020
      nfiles: 30
  train_mutag@gat_w_mean_r_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_w_mean_r_proj
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_w_mean_r_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifier
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadoutsProjectedAndWeightedCombine
          readout_init_args:
            input_dim: ${..repr_dim}
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_w_mean_r_proj
      md5: 26326745dbe6343f58f60c84864930f9.dir
      size: 5611342
      nfiles: 30
  train_mutag@gat_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_mean_pred
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleMeanDecision
          predictor_init_args:
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_mean_pred
      md5: 214e7f530d831655aa34e0abc2844aee.dir
      size: 5165606
      nfiles: 30
  train_mutag@gat_w_mean_pred:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_w_mean_pred
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_w_mean_pred:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_w_mean_pred
      md5: 97be06fd13344f005eefb7a963350fcd.dir
      size: 5339357
      nfiles: 30
  train_mutag@gat_w_mean_pred_proj:
    cmd: PYTHONPATH=. python experiments/scripts/train_gnn_with_reps.py  --config-path
      experiments/config/ensemble_readouts/hparams_mutag.yaml --config-key gat_w_mean_pred_proj
    deps:
    - path: data/datasets/MUTAG
      md5: c96b0b551562ff3ec52c6f642119818c.dir
      size: 454948
      nfiles: 9
    - path: experiments/scripts/train_gnn_with_reps.py
      md5: 94f0e2bc94da01bb49823131310997a7
      size: 2299
    params:
      experiments/config/ensemble_readouts/hparams_mutag.yaml:
        gat_w_mean_pred_proj:
          dataset_dir: data/datasets
          task_level: graph
          dataset_type: tud
          dataset_name: MUTAG
          input_dim: 7
          output_dim: 1
          max_nodes: 28
          repeats: 10
          random_seed:
          - 121371
          - 59211
          - 44185
          - 79709
          - 51612
          - 26233
          - 147
          - 30778
          - 21874
          - 61721
          learning_rate: 0.001
          batch_size: 32
          min_epochs: 10
          max_epochs: 5000
          use_scheduler: true
          scheduler_metric: val_Loss
          lr_scheduler_args:
            min_lr: 1e-06
            mode: min
            factor: 0.5
            patience: 10
            verbose: true
          main_metric: val_F1Score
          early_stopping:
            monitor: val_Loss
            min_delta: 0.0
            patience: 25
            mode: min
          checkpoint:
            monitor: val_Loss
            mode: min
          num_workers: 10
          wandb_project: ensemble-readouts
          experiment_dir: data/experiments/ensemble_readouts/${.dataset_name}/${parent_name:}
          graph_conv: gat
          hidden_dim: 64
          num_layers: 3
          repr_dim: ${.hidden_dim}
          readout_dim: ${.repr_dim}
          predictor_name: MLPClassifierEnsembleProjectedWeightedDecision
          predictor_init_args:
            num_readouts: 3
            input_dim: ${..readout_dim}
            intermediate_dim: 128
            output_dim: ${..output_dim}
            activation: relu
          model_name: GraphModel
          readout_name: MultipleReadouts
          readout_init_args:
            readout_configs:
              AggregateReadoutSum: {}
              AggregateReadoutMean: {}
              AggregateReadoutMax: {}
          conv_kwargs:
            heads: 8
            concat: true
    outs:
    - path: data/experiments/ensemble_readouts/MUTAG/gat_w_mean_pred_proj
      md5: d82855e2209508a0f81a121ab3964b93.dir
      size: 6776972
      nfiles: 30
  summarize_metrics:
    cmd: PYTHONPATH=. python experiments/scripts/summarize_metrics.py  --experiment-root-dir
      data/experiments/ensemble_readouts --n-jobs 20
    deps:
    - path: data/experiments/ensemble_readouts/ENZYMES
      md5: a49acca49040622e6916d56f15720e4f.dir
      size: 2151403060
      nfiles: 1261
    - path: data/experiments/ensemble_readouts/MUTAG
      md5: 373bf14887f4105c11742f1ab8384602.dir
      size: 449642790
      nfiles: 1261
    - path: data/experiments/ensemble_readouts/REDDIT-MULTI-12K
      md5: a168163a23a2d9674910ed1ea2e2df8d.dir
      size: 47903554901
      nfiles: 1261
    - path: data/experiments/ensemble_readouts/zinc
      md5: d4bd0f3c5e9a0338ec0e25e8774fc72d.dir
      size: 1346465457
      nfiles: 1261
    - path: experiments/scripts/summarize_metrics.py
      md5: 4702564c0291945087f9b4ba66d0c0b5
      size: 5095
    outs:
    - path: data/experiments/ensemble_readouts/all_metrics.csv
      md5: 40a37c0da52806f0fa66d16b958c34a7
      size: 197310
    - path: data/experiments/ensemble_readouts/all_metrics_agg.csv
      md5: 5c4988e1e6be4d3ef425a4319be03d21
      size: 41572
    - path: data/experiments/ensemble_readouts/all_metrics_agg.md
      md5: ac50e58f12db0f448732fda917a76b05
      size: 138039
  generate_paper_results:
    cmd: papermill -p results_path data/experiments/ensemble_readouts/all_metrics.csv
      -p plot_path_svg data/experiments/ensemble_readouts/paper/params_vs_efficiency.svg
      -p plot_path_png data/experiments/ensemble_readouts/paper/params_vs_efficiency.png
      -p table_path data/experiments/ensemble_readouts/paper/results_percentage.tex
      notebooks/results_analysis.ipynb data/notebooks/results_analysis_output.ipynb
    deps:
    - path: data/experiments/ensemble_readouts/all_metrics.csv
      md5: 40a37c0da52806f0fa66d16b958c34a7
      size: 197310
    - path: notebooks/results_analysis.ipynb
      md5: 31c6e4bb69c29770ead80f8b9ec9525c
      size: 12618
    outs:
    - path: data/experiments/ensemble_readouts/paper/params_vs_efficiency.png
      md5: f46e39162a8bbae9a98c8559c0be4180
      size: 765833
    - path: data/experiments/ensemble_readouts/paper/params_vs_efficiency.svg
      md5: d2971e17ff74cdf31a33260ae6e42de6
      size: 74881
    - path: data/experiments/ensemble_readouts/paper/results_percentage.tex
      md5: f609ca37f77c5bf335fed43926d1d509
      size: 4329
    - path: data/notebooks/results_analysis_output.ipynb
      md5: cf5197d416c4dce640994c04287e7843
      size: 201944
